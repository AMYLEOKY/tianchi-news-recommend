{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f5db446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import gc, os\n",
    "import logging\n",
    "import time\n",
    "from datetime import datetime\n",
    "import lightgbm as lgb\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3410205a",
   "metadata": {},
   "source": [
    "- 思路の整理概述：\n",
    "> 1. 文章的自身特征: category_id表示这文章的类型， created_at_ts表示文章建立的时间， 这个关系着文章的时效性， words_count是文章的字数， 一般字数太长我们不太喜欢点击, 也不排除有人就喜欢读长文；\n",
    "> 2. 文章的内容embedding特征: 这个召回的时候用过， 这里可以选择使用， 也可以选择不用， 也可以尝试其他类型的embedding特征， 比如W2V等；\n",
    "> 3. 用户的设备特征信息\n",
    "- 上面这些直接可以用的特征， 待做完特征工程之后， 直接就可以根据article_id或者是user_id把这些特征加入进去。 但是我们需要先基于召回的结果，构造一些特征，然后制作标签，形成一个监督学习的数据集。\n",
    "- 构造监督数据集的思路， 根据召回结果， 我们会得到一个{user_id: [可能点击的文章列表]}形式的字典。 那么我们就可以对于每个用户， 每篇可能点击的文章构造一个监督测试集， 比如对于用户user1， 假设得到的他的召回列表{user1: [item1, item2, item3]}， 我们就可以得到三行数据(user1, item1), (user1, item2), (user1, item3)的形式， 这就是监督测试集时候的前两列特征。\n",
    "\n",
    "构造特征的思路如下：\n",
    "构造特征的思路是这样， 我们知道每个用户的点击文章是与其历史点击的文章信息是有很大关联的， 比如同一个主题， 相似等等。 \n",
    "所以特征构造这块很重要的一系列特征是：——————————————————要结合用户的历史点击文章信息。\n",
    "- 目前的召回结果：我们已经得到的就是用户: [item1, item2, item3]列表，以及相应得分；\n",
    "- 而目的是要预测最后一次点击的文章，比较自然的一个思路就是和其最后几次点击的文章产生关系，这样既考虑了其历史点击文章信息，又得离最后一次点击较近；——源于用户兴趣标签可能距离时间相近的更有高权重：\n",
    "> 1. 候选item与最后几次点击的相似性特征(embedding内积） — 这个直接关联用户历史行为\n",
    "> 2. 候选item与最后几次点击的相似性特征的统计特征 — 统计特征可以减少一些波动和异常\n",
    "> 3. 候选item与最后几次点击文章的字数差的特征 — 可以通过字数看用户偏好\n",
    "> 4. 候选item与最后几次点击的文章建立的时间差特征 — 时间差特征可以看出该用户对于文章的实时性的偏好\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55216754",
   "metadata": {},
   "source": [
    "- 最终确定的整体思路：\n",
    "> 1. 我们首先获得用户的最后一次点击操作和用户的历史点击， 这个基于我们的日志数据集做\n",
    "> 2. 基于用户的历史行为制作特征， 这个会用到用户的历史点击表， 最后的召回列表， 文章的信息表和embedding向量\n",
    "> 3. 制作标签， 形成最后的监督学习数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc002eb4",
   "metadata": {},
   "source": [
    "### 目录：\n",
    "- 序章：\n",
    "> 压缩内存的函数&定义数据存储的路径：\n",
    "- C1. 数据前期处理，准备：\n",
    "> 1. S1.获取全数据集；\n",
    "> 2. S2.定义验证集的采样流程；\n",
    "> 3. S3.获取最后和历史点击；\n",
    "> 4. S4.真正的训练、测试、验证集——调用S2中的验证集采样标准；\n",
    "> 5. S5.读取召回列表——这里实际有效的只有ItemCF召回；\n",
    "> 6. S6.读取信息embedding；\n",
    "> 7. S7.读取文章信息；\n",
    "> 8. S8.处理得到的数据；\n",
    "> 9. S9.读取之前计算好的召回列表；\n",
    "- C2. 特征工程环节之一——负采样处理——关键步骤；\n",
    "> 1. P1.召回数据打Label；——转换成监督学习的前提；get_rank_label_df()打标签；\n",
    "> 2. P2.定义负采样函数——neg_sample_recall_data()；\n",
    "> 3. P3.通过召回列表，点击历史，验证历史，测试历史，末次点击训练；get_user_recall_item_label_df()：：\n",
    "> 4. P4.最终的召回做字典存储；\n",
    "- C3. 特征工程环节之二——用户历史行为，结合历史点击过的文章和召回的文章创造一些相似度上+词数+时间上的特征；——关键步骤；\n",
    "- 反应一种用户触发相应的点击行为动作上的相似性；\n",
    "- 最后的形式：user_id、click_article_id、sim0、time_diff0、word_diff0、sim_max、sim_min、sim_sum、sim_mean、sim_median、score、rank、label\n",
    "> 1. P1.读取文章向量和文章基本属性信息：article_info_df = get_article_info_df()，articles_emb = get_embedding(save_path, all_click_df)\n",
    "> 2. P2.基于点击的data做历史相关的特征；基于召回和最后一次点击的文章物品计算相似度，时间差等信息；\n",
    "> 3. P3.获取训练验证及测试数据中召回列文章相关特征；\n",
    "> 4. 做好的历史行为相关特征，进行存储；\n",
    "形式为：user_id-click_article_id-sim0-time_diff0-word_diff0-sim_max-sim_min-sim_sum-sim_mean-sim_median-score-rank-label\n",
    ">> 1. trn_user_item_feats_df = pd.read_csv(save_dir + 'trn_user_item_feats_df.csv')\n",
    ">> 2. tst_user_item_feats_df = pd.read_csv(save_dir + 'tst_user_item_feats_df.csv')\n",
    "- C4. 特征工程环节之三——用户/文章活跃热门特征构造处理；\n",
    "> 1. P1.用户活跃度参数：——基于用户点击文章的篇数，平均间隔时间；\n",
    "> 2. P2.文章热门度参数：——基于文章被点次数，被点时间间隔；\n",
    "- C5. 特征工程环节之四——用户习惯偏好层面的数据挖掘——可以理解为用户画像的处理；\n",
    "- 从all_click_df就是全部点击日志中，可以看到点击环境，时间，设备，地区，类型等信息；这个是不是可以视为所谓的用户画像？\n",
    "- 用户的设备习惯， 这里取最常用的设备，最多的是哪个——取众数；\n",
    "- 用户的时间习惯： 用户习惯啥时候看新闻和物品；\n",
    "- 用户的爱好特征， 文章的类别、主题信息分类；\n",
    "- 用户文章的词数， 用户的爱好文章的字数习惯，偏爱短文章长文章；\n",
    "> 1. P1.用户设备习惯特征挖掘：拿出最多的！ \n",
    "> 2. P2.用户时间习惯特征挖掘：最习惯在几点点击浏览，最喜欢啥时候的！\n",
    "> 3. P3.用户主题类别偏好特征挖掘：点击过啥主题类别的，并且召回的是否在点击的里面，最后做一个布尔值字段,is_cat_hub\n",
    "> 4. P4.用户的文章词数偏好特征挖掘：用户点击过的文章词语数平均值\n",
    "> 5. P5.用户层面的所有信息汇总；保存到指定路径：user_feature_all_info.to_csv(save_dir + 'user_feature_info.csv', index=False)  \n",
    "- 最终字段解释：\n",
    "<!-- 'user_id',           用户id；\n",
    "'click_article_id',     点击文章id；\n",
    "'sim0',             召回的文章物品与最后一次点击的文章之间的相似度；\n",
    "'time_diff0',         召回的文章物品与最后一次点击的文章之间创建时间的差值；\n",
    "'word_diff0',         召回的文章物品与最后一次点击的文章之间文章词数的差值；\n",
    "'sim_max',           用户层面上召回的所有文章物品，与最后一次点击的文章之间的相似度最大值；\n",
    "'sim_min',           用户层面上召回的所有文章物品，与最后一次点击的文章之间的相似度最小值；\n",
    "'sim_sum',           用户层面上召回的所有文章物品，与最后一次点击的文章之间的相似度的Σ值；\n",
    "'sim_mean',          用户层面上召回的所有文章物品，与最后一次点击的文章之间的相似度平均值；\n",
    "'sim_median',         用户层面上召回的所有文章物品，与最后一次点击的文章之间的相似度中位值；\n",
    "'score',            召回的文章物品分数；\n",
    "'rank',             召回的文章物品与最后一次点击的文章之间的排序；\n",
    "'click_size',         用户层面：点击次数取到数\n",
    "'time_diff_mean',      用户层面：点击时间间隔平均值\n",
    "'active_level_parameter', 用户活跃度参数，越大越活跃\n",
    "'click_environment',    该用户最频繁的点击环境\n",
    "'click_deviceGroup',    该用户最频繁的点击设备组\n",
    "'click_os',          该用户最频繁的点击操作系统\n",
    "'click_country',       该用户最频繁的点击城市\n",
    "'click_region',       该用户最频繁的点击地区\n",
    "'click_referrer_type',   该用户最频繁的点击来源类型\n",
    "'click_weekday',       该用户最频繁的点击星期几\n",
    "'click_hour',         该用户最频繁的点击几点\n",
    "'article_crt_weekday',   该用户点击的文章创建最频繁周几\n",
    "'article_crt_hour',     该用户点击的文章创建最频繁几点\n",
    "'words_hbo',          该用户点击的文章词数      \n",
    "'category_id',        该用户点击的文章类别\n",
    "'words_count',        文章的词数\n",
    "'is_cat_hab',         即召回文章的主题类别是否在用户的点击过的主题类别里面：即is_cat_hub字段；\n",
    "'label'                是否是用户最后一次点击；——本项目中的特殊性，因为要求是最后一次点击所以要用最后一次click_trn_hist找到最后一次拼接标签；\n",
    "——如果是正常情况只需要召回的文章物品中看点没点就行；\n",
    "    -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cb2396",
   "metadata": {},
   "source": [
    "# 序章：定义内存节省函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a5c78f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 节约内存的一个标配函数，一种数据压缩技术\n",
    "def reduce_mem(df):\n",
    "    starttime = time.time()\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if pd.isnull(c_min) or pd.isnull(c_max):\n",
    "                continue\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('-- Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction),time spend:{:2.2f} min'.format(end_mem,\n",
    "                                                                                                           100*(start_mem-end_mem)/start_mem,\n",
    "                                                                                                           (time.time()-starttime)/60))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c927bd41",
   "metadata": {},
   "source": [
    "# 序章：定义文件/结果路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ddd0ee2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data_path/'              # 原始数据存放位置\n",
    "save_path = './recall_save_path/'       # 保存一些计算结果的文件，比如物品、用户相似度\n",
    "save_dir = './feature_project_path/'    # 特征工程结果的存放位置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fb6321",
   "metadata": {},
   "source": [
    "# C1.数据前期处理，准备\n",
    "- 在训练集中抽取部分用户的所有信息来作为验证集。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d82e1f7",
   "metadata": {},
   "source": [
    "### S1.获取全数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be6f7880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 之前读取函数原封不动——先读取数据生成all_click_df训练集\n",
    "def get_all_click_df(data_path = data_path, offline=True):\n",
    "    if offline:\n",
    "        all_click = pd.read_csv(data_path + 'train_click_log.csv')\n",
    "    else:\n",
    "        trn_click = pd.read_csv(data_path + 'train_click_log.csv')\n",
    "        tst_click = pd.read_csv(data_path + 'testA_click_log.csv')\n",
    "\n",
    "        all_click = trn_click.append(tst_click)    \n",
    "    all_click = all_click.drop_duplicates((['user_id', 'click_article_id', 'click_timestamp']))\n",
    "    return all_click\n",
    "all_click_df = get_all_click_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6d96b9",
   "metadata": {},
   "source": [
    "### S2.定义验证集的采样流程；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a085957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_click_df指的是训练集\n",
    "# sample_user_nums 采样作为验证集的用户数量\n",
    "def trn_val_split(all_click_df, sample_user_nums):\n",
    "    all_click = all_click_df\n",
    "    all_user_ids = all_click.user_id.unique()\n",
    "    # 随机抽取一部分用户作为验证集的样本；replace=True表示可以重复抽样，反之不可以\n",
    "    sample_user_ids = np.random.choice(all_user_ids, size=sample_user_nums, replace=False) \n",
    "    # 随机采样的作为验证集样本\n",
    "    click_val = all_click[all_click['user_id'].isin(sample_user_ids)]\n",
    "    # 随机采样的以外数据作为训练集样本\n",
    "    click_trn = all_click[~all_click['user_id'].isin(sample_user_ids)]\n",
    "    # 将验证集点击顺序按时间排序\n",
    "    click_val = click_val.sort_values(['user_id', 'click_timestamp'])\n",
    "    # 将验证集中的最后一次点击给抽取出来作为答案结果集\n",
    "    val_ans = click_val.groupby('user_id').tail(1)\n",
    "    ############## 下面一部很关键！\n",
    "    # 去除val_ans中某些用户只有一个点击数据的情况，如果该用户只有一个点击数据，又被分到答案结果集中\n",
    "    # 那么训练集中就没有这个用户的点击数据，出现用户冷启动问题，给自己模型验证带来麻烦\n",
    "    click_val = click_val.groupby('user_id').apply(lambda x: x[:-1]).reset_index(drop=True)\n",
    "    val_ans = val_ans[val_ans.user_id.isin(click_val.user_id.unique())] # 保证答答案结果集中出现的用户在验证集中存在\n",
    "    click_val = click_val[click_val.user_id.isin(val_ans.user_id.unique())]\n",
    "    return click_trn, click_val, val_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f650fa7",
   "metadata": {},
   "source": [
    "### S3.最后和历史点击！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88d84b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取当前数据的历史点击和最后一次点击\n",
    "def get_hist_and_last_click(all_click):\n",
    "    all_click = all_click.sort_values(by=['user_id', 'click_timestamp'])\n",
    "    click_last_df = all_click.groupby('user_id').tail(1)\n",
    "    # 如果用户只有一个点击，hist为空了，会导致训练的时候这个用户不可见，此时默认泄露一下\n",
    "    def hist_func(user_df):\n",
    "        if len(user_df) == 1:\n",
    "            return user_df\n",
    "        else:\n",
    "            return user_df[:-1]\n",
    "    click_hist_df = all_click.groupby('user_id').apply(hist_func).reset_index(drop=True)\n",
    "    return click_hist_df, click_last_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaa47e0",
   "metadata": {},
   "source": [
    "### S4.真正的训练、测试、验证集——调用S2中的验证集采样标准"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5cb729d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取训练、验证及测试集\n",
    "def get_trn_val_tst_data(data_path, offline=True):\n",
    "    if offline:\n",
    "        click_trn_data = pd.read_csv(data_path+'train_click_log.csv')  # 训练集用户点击日志\n",
    "        click_trn_data = reduce_mem(click_trn_data) # 减少内存\n",
    "        click_trn, click_val, val_ans = trn_val_split(click_trn_data , sample_user_nums) # 调用上面编写的采样标准\n",
    "    else:\n",
    "        click_trn = pd.read_csv(data_path+'train_click_log.csv')\n",
    "        click_trn = reduce_mem(click_trn)\n",
    "        click_val = None                                               # 线上比赛就无需这样弄了\n",
    "        val_ans = None                                                 \n",
    "    click_tst = pd.read_csv(data_path+'testA_click_log.csv')           # 最终测试集\n",
    "    return click_trn, click_val, click_tst, val_ans                    # 返回点击训练集、点击验证集、点击测试集、答案结果集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a468d312",
   "metadata": {},
   "source": [
    "### S5.读取召回列表——这里实际有效的只有ItemCF召回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "add3a07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 返回多路召回列表或者单路召回\n",
    "def get_recall_list(save_path, single_recall_model=None, multi_recall=False):\n",
    "    if multi_recall:\n",
    "        return pickle.load(open(save_path + 'final_recall_items_dict.pkl', 'rb'))\n",
    "    if single_recall_model == 'i2i_itemcf':\n",
    "        return pickle.load(open(save_path + 'itemcf_recall_dict.pkl', 'rb'))\n",
    "    elif single_recall_model == 'i2i_emb_itemcf':\n",
    "        return pickle.load(open(save_path + 'itemcf_emb_dict.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb34e37",
   "metadata": {},
   "source": [
    "### S6.读取信息embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b5bceede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以通过字典查询对应的item的Embedding\n",
    "def get_embedding(save_path, all_click_df):\n",
    "    if os.path.exists(save_path + 'item_content_emb.pkl'): ## 读取前文获得的文章向量信息\n",
    "        item_content_emb_dict = pickle.load(open(save_path + 'item_content_emb.pkl', 'rb'))\n",
    "    else:\n",
    "        print('item_content_emb.pkl 文件不存在...')\n",
    "#     # w2v Embedding是需要提前训练好的\n",
    "#     if os.path.exists(save_path + 'emb_i2i_sim.pkl'):      ## 读取处理好的物品itemEmbedding信息\n",
    "#         item_w2v_emb_dict = pickle.load(open(save_path + 'emb_i2i_sim.pkl', 'rb'))\n",
    "#     else:\n",
    "#         item_w2v_emb_dict = trian_item_word2vec(all_click_df)    \n",
    "#     return item_content_emb_dict, item_w2v_emb_dict, item_youtube_emb_dict, user_youtube_emb_dict\n",
    "    return item_content_emb_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783c9c92",
   "metadata": {},
   "source": [
    "### S7.读取文章信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5a4f73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_info_df():\n",
    "    article_info_df = pd.read_csv(data_path + 'articles.csv')\n",
    "    article_info_df = reduce_mem(article_info_df)\n",
    "    return article_info_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffc7d35",
   "metadata": {},
   "source": [
    "### S8.处理得到的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "77a4674f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Mem. usage decreased to 21.99 Mb (75.0% reduction),time spend:0.00 min\n"
     ]
    }
   ],
   "source": [
    "# 这里offline的online的区别就是验证集是否为空\n",
    "# 调用S4编写的真正的训练、测试、验证集函数\n",
    "click_trn, click_val, click_tst, val_ans = get_trn_val_tst_data(data_path, offline=False)\n",
    "click_trn_hist, click_trn_last = get_hist_and_last_click(click_trn)\n",
    "if click_val is not None:  # 如果click_val就是点击验证集非空，说明是offline模式\n",
    "    click_val_hist, click_val_last = click_val, val_ans\n",
    "else:                       # 如果click_val就是点击验证集为空，说明是online模式\n",
    "    click_val_hist, click_val_last = None, None  \n",
    "click_tst_hist = click_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb9c127a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>click_article_id</th>\n",
       "      <th>click_timestamp</th>\n",
       "      <th>click_environment</th>\n",
       "      <th>click_deviceGroup</th>\n",
       "      <th>click_os</th>\n",
       "      <th>click_country</th>\n",
       "      <th>click_region</th>\n",
       "      <th>click_referrer_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>157507</td>\n",
       "      <td>1.510000e+12</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>289197</td>\n",
       "      <td>1.510000e+12</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>168401</td>\n",
       "      <td>1.510000e+12</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>50644</td>\n",
       "      <td>1.510000e+12</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>42567</td>\n",
       "      <td>1.510000e+12</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196109</th>\n",
       "      <td>199995</td>\n",
       "      <td>336476</td>\n",
       "      <td>1.510000e+12</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196110</th>\n",
       "      <td>199996</td>\n",
       "      <td>299697</td>\n",
       "      <td>1.510000e+12</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196111</th>\n",
       "      <td>199997</td>\n",
       "      <td>224171</td>\n",
       "      <td>1.510000e+12</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196112</th>\n",
       "      <td>199998</td>\n",
       "      <td>336250</td>\n",
       "      <td>1.510000e+12</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196113</th>\n",
       "      <td>199999</td>\n",
       "      <td>218355</td>\n",
       "      <td>1.510000e+12</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>197496 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id  click_article_id  click_timestamp  click_environment  \\\n",
       "0             0            157507     1.510000e+12                  4   \n",
       "1             1            289197     1.510000e+12                  4   \n",
       "2             2            168401     1.510000e+12                  4   \n",
       "3             3             50644     1.510000e+12                  4   \n",
       "4             4             42567     1.510000e+12                  4   \n",
       "...         ...               ...              ...                ...   \n",
       "196109   199995            336476     1.510000e+12                  4   \n",
       "196110   199996            299697     1.510000e+12                  4   \n",
       "196111   199997            224171     1.510000e+12                  4   \n",
       "196112   199998            336250     1.510000e+12                  4   \n",
       "196113   199999            218355     1.510000e+12                  4   \n",
       "\n",
       "        click_deviceGroup  click_os  click_country  click_region  \\\n",
       "0                       1        17              1            25   \n",
       "1                       1        17              1            25   \n",
       "2                       3        20              1            25   \n",
       "3                       3         2              1            25   \n",
       "4                       1        12              1            16   \n",
       "...                   ...       ...            ...           ...   \n",
       "196109                  1        17              1            25   \n",
       "196110                  1        17              1            25   \n",
       "196111                  1        17              1            16   \n",
       "196112                  1        17              1            25   \n",
       "196113                  1        17              1            13   \n",
       "\n",
       "        click_referrer_type  \n",
       "0                         2  \n",
       "1                         6  \n",
       "2                         2  \n",
       "3                         2  \n",
       "4                         1  \n",
       "...                     ...  \n",
       "196109                    2  \n",
       "196110                    2  \n",
       "196111                    1  \n",
       "196112                    5  \n",
       "196113                    1  \n",
       "\n",
       "[197496 rows x 9 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "click_trn_last"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd41d429",
   "metadata": {},
   "source": [
    "- click_trn    ：点击训练数据集 \n",
    "- click_val    ：线下模式的验证集——这里为空，因为是online\n",
    "- click_tst    ：点击训练测试集\n",
    "- val_ans     ：线下模式做验证的答案结果集——这里为空，因为是online\n",
    "- click_trn_hist：点击历史训练数据集\n",
    "- click_trn_last：点击末次训练数据集\n",
    "- click_val_hist：线上模式历史训练数据 \n",
    "- click_val_last：线上模式末次训练数据\n",
    "- click_tst_hist：点击历史测试数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14dd24a",
   "metadata": {},
   "source": [
    "### S9.读取之前计算好的召回列表\n",
    "——这里选取的是ItemCF召回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "010e188b",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_list_dict = get_recall_list(save_path, single_recall_model='i2i_itemcf')\n",
    "# recall_list_dict = pickle.load(open(save_path + 'final_recall_items_dict.pkl', 'rb')) #两种读取的返回结果有点儿区别"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b3ed35",
   "metadata": {},
   "source": [
    "- 为了方便起见，将召回列表字典改成Dataframe形式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9ec4f225",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 247496/247496 [00:05<00:00, 48328.79it/s]\n"
     ]
    }
   ],
   "source": [
    "def change_recall_list_df(recall_list_dict):\n",
    "    df_row_list = [] # [user, item, score]\n",
    "    for user, recall_list in tqdm(recall_list_dict.items()):\n",
    "        for item, score in recall_list:\n",
    "            df_row_list.append([user, item, score])\n",
    "    column_names = ['user_id', 'sim_item', 'score']\n",
    "    recall_list_df = pd.DataFrame(df_row_list, columns=column_names)\n",
    "    return recall_list_df\n",
    "recall_list_df = change_recall_list_df(recall_list_dict)\n",
    "# 小处理一下召回数据集\n",
    "need_append_info_fortag = pd.DataFrame(click_trn_last.iloc[:,:2])\n",
    "need_append_info_fortag.columns = [\"user_id\",\"sim_item\"]\n",
    "need_append_info_fortag[\"score\"] = 1.63513147 \n",
    "need_append_info_fortag[\"score\"] = need_append_info_fortag[\"score\"].apply(lambda x:x + (np.random.random(1)/1000)[0])\n",
    "recall_list_df=recall_list_df.append(need_append_info_fortag)\n",
    "recall_list_df=recall_list_df.sort_values([\"user_id\",\"score\"])\n",
    "# 恢复索引\n",
    "recall_list_df.index = range(recall_list_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e333fffa",
   "metadata": {},
   "source": [
    "# C2. 特征工程环节之一——负采样处理；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fb251c",
   "metadata": {},
   "source": [
    "## P1.召回数据需要打Label；——转换成监督学习的前提；\n",
    "在click_hr_last中做些处理，结合物品相似度进行join；\n",
    "但是在该场景的情况下，正负样本明显失衡，需要进行一些处理；\n",
    "*** 很关键的一步骤：打标签；\n",
    "- 函数get_rank_label_df()；传入参数：\n",
    "> 1. recall_list_df：已经处理成Dataframe形式的召回列表：user_id,item,score...\n",
    "> 2. label_df：这里其实是click_trn_last，就是每个用户最后一次点击的信息，如果和上述recall_list_df以[user_id，item]联合关联，有结果的话，说明召回有过点击（本案例是最后一次所以要去单独拿出click_trn_last做最后一次区分；），为正样本；否则就是负样本，未曾点击；\n",
    "> 3. 最后返回user_id,sim_item,score,label形式的dataframe；\n",
    "> 4. 就是在recall_list_df的基础上去添加了正负样本标签而已；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e83b8c13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>sim_item</th>\n",
       "      <th>score</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>211442</td>\n",
       "      <td>0.284456</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>299499</td>\n",
       "      <td>0.297558</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>285869</td>\n",
       "      <td>0.313568</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>203336</td>\n",
       "      <td>0.352849</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>284220</td>\n",
       "      <td>0.368981</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  sim_item     score  label\n",
       "0        0    211442  0.284456    0.0\n",
       "1        0    299499  0.297558    0.0\n",
       "2        0    285869  0.313568    0.0\n",
       "3        0    203336  0.352849    0.0\n",
       "4        0    284220  0.368981    0.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_rank_label_df(recall_list_df, label_df, is_test=False):\n",
    "    '''\n",
    "    传入参数：\n",
    "    recall_list_df：已经处理成Dataframe形式的召回列表：user_id,item,score...\n",
    "    label_df：这里其实是click_trn_last，就是每个用户最后一次点击的信息，如果和上述recall_list_df以[user_id，item]联合关联，\n",
    "              有结果的话，说明召回有过点击（本案例是最后一次所以要去单独拿出click_trn_last做最后一次区分；），为正样本；\n",
    "              否则就是负样本，未曾点击；\n",
    "    '''\n",
    "    # 测试集是没有标签了，为了后面代码同一一些，先给一个默认值-1\n",
    "    if is_test:\n",
    "        recall_list_df['label'] = -1\n",
    "        return recall_list_df\n",
    "    label_df = label_df.rename(columns={'click_article_id': 'sim_item'}) # 将点击文章click_article_id重命名成sim_item\n",
    "    # 在召回列表df上拼接新的信息，用户点击时间，点击文章，用户ID，拼接对象是每个用户的末次点击训练集；\n",
    "    # 作为标签儿判断，click_timestamp为空就是0，1就是点击过\n",
    "    recall_list_df_ = recall_list_df.merge(label_df[['user_id', 'sim_item', 'click_timestamp']], how='left', on=['user_id', 'sim_item'])\n",
    "    # 0 表示末次未点，1表示末次点了；\n",
    "    recall_list_df_['label'] = recall_list_df_['click_timestamp'].apply(lambda x: 0.0 if np.isnan(x) else 1.0)\n",
    "    # 把不用的列删除掉！\n",
    "    del recall_list_df_['click_timestamp']\n",
    "    return recall_list_df_\n",
    "recall_user_item_label_df = get_rank_label_df(recall_list_df, click_trn_last)\n",
    "recall_user_item_label_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53a4aa37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    4949920\n",
       "1.0     197496\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_user_item_label_df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce6099a",
   "metadata": {},
   "source": [
    "## P2.负采样函数——后续要用；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1423aa1",
   "metadata": {},
   "source": [
    "- 函数：neg_sample_recall_data()\n",
    "- 在刚才已经打过标签的recall_user_item_label_df基础上进行负采样；\n",
    "- 目的是：在每个用户的带有标签的召回信息中，负样本数，也就是未点击的用户-物品关联至少有1，最多有5个；分别从用户+物品的角度去处理\n",
    "- 最后合并，再去重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c71abd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_sample_recall_data(recall_user_item_label_df, sample_rate=0.001): #采样处理的比例\n",
    "    pos_data = recall_user_item_label_df[recall_user_item_label_df['label'] == 1] ### 正样本标签\n",
    "    neg_data = recall_user_item_label_df[recall_user_item_label_df['label'] == 0] ### 负样本标签\n",
    "    # 临时打印一下正负样本的相互比例；\n",
    "    print('pos_data_num:', len(pos_data), 'neg_data_num:', len(neg_data), 'pos/neg:', len(pos_data)/len(neg_data)) \n",
    "    # 分组采样函数，防止出现没有负样本出现的情况；\n",
    "    # 其实是对label为0的召回信息+标签部分进行处理；\n",
    "    def neg_sample_func(neg_part_group_df):\n",
    "        neg_num = len(neg_part_group_df) \n",
    "        sample_num = max(int(neg_num * sample_rate), 1) # 保证最少有一个\n",
    "        sample_num = min(sample_num, 5) # 保证最多不超过5个，这里可以根据实际情况进行选择\n",
    "        return neg_part_group_df.sample(n=sample_num, replace=True)\n",
    "    # 对用户进行负采样，保证所有用户都在采样后的数据中\n",
    "    neg_data_user_sample = neg_data.groupby('user_id', group_keys=False).apply(neg_sample_func) #group_keys=False防止篡改原来索引\n",
    "    # 对文章进行负采样，保证所有文章都在采样后的数据中\n",
    "    neg_data_item_sample = neg_data.groupby('sim_item', group_keys=False).apply(neg_sample_func)\n",
    "    # 将上述两种情况下的采样数据合并\n",
    "    neg_data_new = neg_data_user_sample.append(neg_data_item_sample)\n",
    "    # 由于上述两个操作是分开的，可能将两个相同的数据给重复选择了，所以需要对合并后的数据进行去重\n",
    "    neg_data_new = neg_data_new.sort_values(['user_id', 'score']).drop_duplicates(['user_id', 'sim_item'], keep='last')\n",
    "    # 将正样本数据合并\n",
    "    data_new = pd.concat([pos_data, neg_data_new], ignore_index=True)   \n",
    "    return data_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072cd5ee",
   "metadata": {},
   "source": [
    "## P3.通过召回列表，点击历史，验证历史，测试历史，末次点击训练\n",
    "- 函数：get_user_recall_item_label_df()：\n",
    "> 1. 将上述的打标签函数 + 负采样函数结合到一起；\n",
    "> 2. 同时对测试集中的用户也进行标签操作；\n",
    "> 3. 最终得到\n",
    ">> 1. S1. trn_user_item_label_df：训练用的样本，用户召回物品标签\n",
    ">> 2. S2. val_user_item_label_df：验证用的样本，用户召回物品标签——这里为空；\n",
    ">> 3. S3. tst_user_item_label_df：测试用的样本，用户召回物品标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6de27c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4147416, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_user_items_df = recall_list_df[recall_list_df['user_id'].isin(click_trn_hist['user_id'].unique())]\n",
    "trn_user_items_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "325c3c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4147416, 4),\n",
       " 0.0    3949920\n",
       " 1.0     197496\n",
       " Name: label, dtype: int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 打标签环节：\n",
    "trn_user_item_label_df = get_rank_label_df(trn_user_items_df, click_trn_last, is_test=False)\n",
    "trn_user_item_label_df.shape,trn_user_item_label_df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7329060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_data_num: 197496 neg_data_num: 3949920 pos/neg: 0.05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((410496, 4),\n",
       " 0.0    213000\n",
       " 1.0    197496\n",
       " Name: label, dtype: int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 负采样处理：\n",
    "trn_user_item_label_df = neg_sample_recall_data(trn_user_item_label_df)\n",
    "trn_user_item_label_df.shape,trn_user_item_label_df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a6ef295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可减负样本数量比例发生了变化！样本间更见平衡一些。。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2461ad86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_recall_item_label_df(click_trn_hist, click_val_hist, click_tst_hist,click_trn_last, click_val_last, recall_list_df):\n",
    "    # 获取训练数据的召回列表,就是——从训练点击历史集的所有用户他们的召回列表信息，当然不能用click_trn_last。。。人家是用来打标签的\n",
    "    trn_user_items_df = recall_list_df[recall_list_df['user_id'].isin(click_trn_hist['user_id'].unique())]\n",
    "    # 训练数据打标签\n",
    "    trn_user_item_label_df = get_rank_label_df(trn_user_items_df, click_trn_last, is_test=False)\n",
    "    # 训练数据负采样\n",
    "    trn_user_item_label_df = neg_sample_recall_data(trn_user_item_label_df)\n",
    "    \n",
    "    if click_val is not None:\n",
    "        val_user_items_df = recall_list_df[recall_list_df['user_id'].isin(click_val_hist['user_id'].unique())]\n",
    "        val_user_item_label_df = get_rank_label_df(val_user_items_df, click_val_last, is_test=False)\n",
    "        val_user_item_label_df = neg_sample_recall_data(val_user_item_label_df)\n",
    "    # 走的是下面的这一段；所以val_user_item_label_df为空\n",
    "    else:\n",
    "        val_user_item_label_df = None\n",
    "    # 测试数据不需要进行负采样，直接对所有的召回商品进行打-1标签\n",
    "    tst_user_items_df = recall_list_df[recall_list_df['user_id'].isin(click_tst_hist['user_id'].unique())]\n",
    "    tst_user_item_label_df = get_rank_label_df(tst_user_items_df, None, is_test=True)\n",
    "    return trn_user_item_label_df, val_user_item_label_df, tst_user_item_label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b09bca53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_data_num: 197496 neg_data_num: 3949920 pos/neg: 0.05\n"
     ]
    }
   ],
   "source": [
    "# 给训练验证数据打标签，并负采样（这一部分时间比较久）\n",
    "trn_user_item_label_df, val_user_item_label_df, tst_user_item_label_df = get_user_recall_item_label_df(click_trn_hist, \n",
    "                                                                                                       click_val_hist, \n",
    "                                                                                                       click_tst_hist,\n",
    "                                                                                                       click_trn_last, \n",
    "                                                                                                       click_val_last, \n",
    "                                                                                                       recall_list_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bfb1ae6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((410486, 4), None, (1000000, 4))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_user_item_label_df.shape,val_user_item_label_df,tst_user_item_label_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4075693",
   "metadata": {},
   "source": [
    "## P4.最终的召回做字典存储；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b72eae6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 将最终的召回的df数据转换成字典的形式做排序特征\n",
    "def make_tuple_func(group_df):\n",
    "    row_data = []\n",
    "    # 遍历上面三个集合，把物品，分数，标签拿过来；分数做以下处理太长了\n",
    "    for name, row_df in group_df.iterrows():\n",
    "        row_data.append((row_df['sim_item'], round(row_df['score'],6), row_df['label']))\n",
    "    return row_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8c3f6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练集用的部分；以user为索引groupby后再复原勿忘\n",
    "trn_user_item_label_tuples = trn_user_item_label_df.groupby('user_id').apply(make_tuple_func).reset_index()\n",
    "trn_user_item_label_tuples_dict = dict(zip(trn_user_item_label_tuples['user_id'], trn_user_item_label_tuples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "062971df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证集和测试集同样的处理法则；\n",
    "if val_user_item_label_df is not None:\n",
    "    val_user_item_label_tuples = val_user_item_label_df.groupby('user_id').apply(make_tuple_func).reset_index()\n",
    "    val_user_item_label_tuples_dict = dict(zip(val_user_item_label_tuples['user_id'], val_user_item_label_tuples[0]))\n",
    "else:\n",
    "    val_user_item_label_tuples_dict = None\n",
    "    \n",
    "tst_user_item_label_tuples = tst_user_item_label_df.groupby('user_id').apply(make_tuple_func).reset_index()\n",
    "tst_user_item_label_tuples_dict = dict(zip(tst_user_item_label_tuples['user_id'], tst_user_item_label_tuples[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ec64ba4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trn_user_item_label_tuples_dict,tst_user_item_label_tuples_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c7a489",
   "metadata": {},
   "source": [
    "# C3.特征工程环节之二——用户历史行为，结合历史点击过的文章和召回的文章创造一些相似度上+词数+时间上的特征；\n",
    "- 因为在浏览新闻的环节中，用户对于文章长短有偏少词数的倾向；并且新闻浏览的时效性比传统物品的主题浏览的偏好性会更强；所以看一下相似度词数创建时间等特征信息；\n",
    "- 再一个很关键的环节；\n",
    "- 函数create_feature()\n",
    "- 前提之一：调用上面编写好的函数get_article_info_df()获取文章的信息article_info_df；\n",
    "- 前提之二：click_trn_hist，click_tst_hist；\n",
    "- 前提之三：上述已创建好的trn_user_item_label_tuples_dict,tst_user_item_label_tuples_dict；\n",
    "- 物品embedding信息，get_embedding()函数，为了计算召回物品与最终点击物品的相似度；\n",
    "- 针对上述划分好的click_trn_hist，click_tst_hist，上述已创建好的trn_user_item_label_tuples_dict,tst_user_item_label_tuples_dict；\n",
    "分别做特征创造；\n",
    "- 目的梳理清楚；针对每一个用户遍历，在其召回物品列表，分数，标签值的基础上，结合其点击历史信息；\n",
    "> 1. 遍历每一个用户：从其召回列表信息中，即文章物品——分数——标签；拿到该文章物品的时间，词数等信息；\n",
    "并生成该用户每个召回文章物品的列表信息[user_id, item]——后续基础信息结构搭建；\n",
    "> 2. 遍历该用户访问的最后N次的文章物品（通常是最后一次），分别计算与召回文章物品的相似度，创建时间，词数差等信息；\n",
    "追加加到[user_id, item]后面中，并添加相似度的最大最小平均中位值等额外信息，打分，标签不能丢；\n",
    "> 3.所有的信息添加到最终的all_user_feas做Dataframe处理\n",
    "- 最后的形式：user_id、click_article_id、sim0、time_diff0、word_diff0、sim_max、sim_min、sim_sum、sim_mean、sim_median、score、rank、label\n",
    "- 得到的字段解释：\n",
    "sim0：      召回的文章物品与最后一次点击的文章之间的相似度；\n",
    "time_diff0：  召回的文章物品与最后一次点击的文章之间创建时间的差值；\n",
    "word_diff0：  召回的文章物品与最后一次点击的文章之间文章词数的差值；\n",
    "sim_max：    用户层面上召回的所有文章物品，与最后一次点击的文章之间的相似度最大值；\n",
    "sim_min：    用户层面上召回的所有文章物品，与最后一次点击的文章之间的相似度最小值；\n",
    "sim_sum：    用户层面上召回的所有文章物品，与最后一次点击的文章之间的相似度的Σ值；\n",
    "sim_mean：   用户层面上召回的所有文章物品，与最后一次点击的文章之间的相似度平均值；\n",
    "sim_median：  用户层面上召回的所有文章物品，与最后一次点击的文章之间的相似度中位值；\n",
    "score：     召回的文章物品分数；\n",
    "rank：      召回的文章物品与最后一次点击的文章之间的排序；\n",
    "label：     是否是用户最后一次点击；——本项目中的特殊性，因为要求是最后一次点击所以要用最后一次click_trn_hist找到最后一次拼接标签；\n",
    "——如果是正常情况只需要召回的文章物品中看点没点就行；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7e4748",
   "metadata": {},
   "source": [
    "## P1.读取文章向量和文章基本属性信息 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a05209a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Mem. usage decreased to  0.25 Mb (50.0% reduction),time spend:0.00 min\n"
     ]
    }
   ],
   "source": [
    "article_info_df = get_article_info_df()\n",
    "articles_emb = get_embedding(save_path, all_click_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c849dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click_trn_hist[click_trn_hist[\"user_id\"]==163549]\n",
    "# trn_user_item_label_tuples_dict[163549]\n",
    "# # 建立一个二维列表保存结果， 后面要转成DataFrame，这里选取任意一个用户看一下流程,163549这个用户和训练集trn_user_item_label_tuples_dict\n",
    "# all_user_feas = []\n",
    "# i = 0\n",
    "# # 获取了该用户的最后一次点击的物品，该用户最后一次点击文章是211732\n",
    "# hist_user_items_163549 = click_trn_hist[click_trn_hist['user_id']==163549]['click_article_id'][-1:]\n",
    "# # 遍历该用户的召回列表，即召回文章、分数、标签：该用户召回是285663+205866+202534三篇\n",
    "# for rank, (article_id, score, label) in enumerate(trn_user_item_label_tuples_dict[163549]):\n",
    "#     # 召回的文章物品创建时间\n",
    "#     recall_items_create_time = article_info_df[article_info_df['article_id']==article_id]['created_at_ts'].values[0] \n",
    "#     # 召回的文章物品词数信息\n",
    "#     recall_items_words_count = article_info_df[article_info_df['article_id']==article_id]['words_count'].values[0]\n",
    "#     single_user_fea = [163549, article_id]\n",
    "#     # 计算与最后点击的商品的相似度的和， 最大值和最小值， 均值    \n",
    "#     sim_fea = []                                     # sim_fea：相似度特征\n",
    "#     time_fea = []                                    # time_fea：时间特征\n",
    "#     words_fea = []                                   # word_fea词数差特征\n",
    "#     # 遍历用户的最后N次点击文章，这里是最后一次,比较一下召回的文章物品在创建时间、字数、相似度上的差距，所以就1个\n",
    "#     for hist_item in hist_user_items_163549:\n",
    "#         b_create_time = article_info_df[article_info_df['article_id']==hist_item]['created_at_ts']. values[0]\n",
    "#         b_words_count = article_info_df[article_info_df['article_id']==hist_item]['words_count'].values[0]\n",
    "#         sim_fea.append(cosine_similarity([articles_emb[hist_item],articles_emb[article_id]])[0][1])            # 计算召回文章与末N次点击文章的相似度\n",
    "#         time_fea.append(abs(recall_items_create_time-b_create_time))                                           # 计算召回文章与末N次点击文章的时间差\n",
    "#         words_fea.append(abs(recall_items_words_count-b_words_count))                                          # 计算召回文章与末N次点击文章的词数差\n",
    "#     single_user_fea.extend(sim_fea)      # 相似性特征\n",
    "#     single_user_fea.extend(time_fea)    # 时间差特征\n",
    "#     single_user_fea.extend(words_fea)    # 字数差特征\n",
    "#     single_user_fea.extend([max(sim_fea), min(sim_fea), sum(sim_fea), sum(sim_fea) / len(sim_fea),np.median(sim_fea)])  # 相似性的统计特征\n",
    "#     single_user_fea.extend([score, rank, label])      \n",
    "#     all_user_feas.append(single_user_fea)\n",
    "# all_user_feas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd374d14",
   "metadata": {},
   "source": [
    "## P2.下面基于data做历史相关的特征，这里去拿最后1次点击；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c341937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义历史特征相关计算函数\n",
    "def create_feature(users_id, recall_list, click_hist_df,  articles_info, articles_emb, user_emb=None, N=1):\n",
    "    \"\"\"\n",
    "    基于用户的历史行为做相关特征\n",
    "    : users_id: 用户id\n",
    "    : recall_list: 对于每个用户召回的候选文章列表，候选值：trn_user_item_label_tuples_dict&tst_user_item_label_tuples_dict\n",
    "    : click_hist_df: 用户的历史点击信息，候选值：click_trn_hist&click_tst_hist\n",
    "    : articles_info: 文章信息\n",
    "    : articles_emb: 文章的embedding向量, 这个可以用item_content_emb, item_w2v_emb, item_youtube_emb\n",
    "    :param N: 最近的N次点击  由于testA日志里面很多用户只存在一次历史点击， 所以为了不产生空值，默认是1\n",
    "    \"\"\"\n",
    "    # 建立一个二维列表保存结果， 后面要转成DataFrame\n",
    "    all_user_feas = []\n",
    "    i = 0\n",
    "    for user_id in tqdm(users_id):\n",
    "        # 该用户的最后N次点击，这里是最后1次采用；\n",
    "        hist_user_items = click_hist_df[click_hist_df['user_id']==user_id]['click_article_id'][-N:]\n",
    "        # 遍历该用户的召回列表\n",
    "        for rank, (article_id, score, label) in enumerate(recall_list[user_id]):\n",
    "            # 该文章建立时间, 字数\n",
    "            recall_item_create_time = articles_info[articles_info['article_id']==article_id]['created_at_ts'].values[0]\n",
    "            recall_item_words_count = articles_info[articles_info['article_id']==article_id]['words_count'].values[0]\n",
    "            single_user_fea = [user_id, article_id]\n",
    "            # 计算与最后点击的商品的相似度，聚合运算其相似度的和， 最大值和最小值， 均值，中位值\n",
    "            sim_fea = []\n",
    "            time_fea = []\n",
    "            words_fea = []\n",
    "            # 遍历用户的最后N次点击文章\n",
    "            for hist_item in hist_user_items:\n",
    "                final_event_create_time = articles_info[articles_info['article_id']==hist_item]['created_at_ts'].values[0]\n",
    "                final_event_words_count = articles_info[articles_info['article_id']==hist_item]['words_count'].values[0]\n",
    "                sim_fea.append(np.dot(articles_emb[hist_item], articles_emb[article_id]))\n",
    "                time_fea.append(abs(recall_item_create_time-final_event_create_time))\n",
    "                words_fea.append(abs(recall_item_words_count-final_event_words_count))\n",
    "            single_user_fea.extend(sim_fea)      # 相似性特征\n",
    "            single_user_fea.extend(time_fea)    # 时间差特征\n",
    "            single_user_fea.extend(words_fea)    # 字数差特征\n",
    "            single_user_fea.extend([max(sim_fea), min(sim_fea), sum(sim_fea), sum(sim_fea) / len(sim_fea), np.median(sim_fea)])  # 相似性的统计特征\n",
    "            single_user_fea.extend([score, rank, label])    \n",
    "            # 加入到总的表中\n",
    "            all_user_feas.append(single_user_fea)\n",
    "    # 定义列名\n",
    "    id_cols = ['user_id', 'click_article_id']\n",
    "    sim_cols = ['sim' + str(i) for i in range(N)]\n",
    "    time_cols = ['time_diff' + str(i) for i in range(N)]\n",
    "    word_cols = ['word_diff' + str(i) for i in range(N)]\n",
    "    sat_cols = ['sim_max', 'sim_min', 'sim_sum', 'sim_mean' , 'sim_median']\n",
    "    user_item_sim_cols = ['user_item_sim'] if user_emb else []\n",
    "    user_score_rank_label = ['score', 'rank', 'label']\n",
    "    cols = id_cols + sim_cols + time_cols + word_cols + sat_cols + user_item_sim_cols + user_score_rank_label\n",
    "    # 转成DataFrame\n",
    "    df = pd.DataFrame(all_user_feas, columns=cols)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0856a803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_163549_info = create_feature([163549], trn_user_item_label_tuples_dict, click_trn_hist, article_info_df,articles_emb, user_emb=None, N=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa74729",
   "metadata": {},
   "source": [
    "## P3. 获取训练验证及测试数据中召回列文章相关特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "be80df69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 197496/197496 [17:03<00:00, 192.99it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 50000/50000 [28:58<00:00, 28.77it/s]\n"
     ]
    }
   ],
   "source": [
    "trn_user_item_feats_df = create_feature(trn_user_item_label_tuples_dict.keys(), trn_user_item_label_tuples_dict, \\\n",
    "                                            click_trn_hist, article_info_df, articles_emb)\n",
    "if val_user_item_label_tuples_dict is not None:\n",
    "    val_user_item_feats_df = create_feature(val_user_item_label_tuples_dict.keys(), val_user_item_label_tuples_dict, \\\n",
    "                                                click_val_hist, article_info_df, articles_emb)\n",
    "else:\n",
    "    val_user_item_feats_df = None\n",
    "tst_user_item_feats_df = create_feature(tst_user_item_label_tuples_dict.keys(), tst_user_item_label_tuples_dict, \\\n",
    "                                            click_tst_hist, article_info_df, articles_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe1452f",
   "metadata": {},
   "source": [
    "- 如果未生成新的特征工程处理的数据，需要运行以下代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c93cdb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 保存一份省的每次都要重新跑，每次跑的时间都比较长\n",
    "# trn_user_item_feats_df.to_csv(save_dir + 'trn_user_item_feats_df.csv', index=False)\n",
    "\n",
    "# if val_user_item_feats_df is not None:\n",
    "#     val_user_item_feats_df.to_csv(save_dir + 'val_user_item_feats_df.csv', index=False)\n",
    "\n",
    "# tst_user_item_feats_df.to_csv(save_dir + 'tst_user_item_feats_df.csv', index=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d337607",
   "metadata": {},
   "source": [
    "- 如果已生成新的特征工程处理的数据，需要运行以下代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f39fdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_user_item_feats_df = pd.read_csv(save_dir + 'trn_user_item_feats_df.csv')\n",
    "tst_user_item_feats_df = pd.read_csv(save_dir + 'tst_user_item_feats_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b97fb0f",
   "metadata": {},
   "source": [
    "# C4.特征工程环节之三——用户/文章特征构造处理；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6711f3d6",
   "metadata": {},
   "source": [
    "- 有几个方面可以考虑：\n",
    "> 1. 用户点击文章次数+点击时间构造，可以表现用户活跃度：前面召回阶段已经有过初步判断；\n",
    "> 2. 文章物品被点击次数+被点击时间构造可以反映文章热度的特征；\n",
    "> 3. 用户的浏览时间偏好性： 根据其点击的历史文章列表的点击时间和文章的创建时间做统计特征，比如求均值， 这个可以反映用户对于文章时效的偏好；\n",
    "> 4. 用户的主题偏好性， 对于用户点击的历史文章主题进行一个统计， 然后对于当前文章看看是否属于用户已经点击过的主题；\n",
    "> 5. 用户的字数偏好性特征， 对于用户点击的历史文章的字数统计， 求一个均值，看看用户喜欢长文短文；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2040be23",
   "metadata": {},
   "source": [
    "## P0.准备阶段——文章和用户信息整合：\n",
    "- all_data：用户点击+文章的信息联合df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e1f898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# article_info_df前面已经读取完毕，进行一下内存减少处理，更名articles\n",
    "articles = reduce_mem(article_info_df)\n",
    "# 训练集测试集一起\n",
    "if click_val is not None:\n",
    "    all_data = click_trn.append(click_val)\n",
    "all_data = click_trn.append(click_tst)\n",
    "# 加入文章信息，注意click_article_id和article_id两列相同；\n",
    "all_data = all_data.merge(articles, left_on='click_article_id', right_on='article_id')\n",
    "all_data = reduce_mem(all_data)\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc510adf",
   "metadata": {},
   "source": [
    "## P1.用户活跃度的判断；\n",
    "- 分析一下每个用户点击时间和点击文章的次数，区分用户活跃度；\n",
    "> 1. 用户点击行为之间时间间隔差距小，并且点击次数多，说明活跃程度高；\n",
    "> 2. 对于每个用户，计算点击文章的次数， 两两点击文章时间间隔的均值；\n",
    "> 3. 把点击次数&时间间隔的倒数的均值统一归一化，然后两者相加加和，该值越大，说明用户越活跃；——原版是越小越活跃太难受了调过来！\n",
    "> 4. 最终得到的一列是user_id,click_size次数的倒数，time_diff_mean平均点击时间间隔，active_level_parameter活跃度参数！\n",
    "- 最终结果：user_act_fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e0e8c4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# activition_need_cols = ['user_id', 'click_article_id', 'click_timestamp']\n",
    "# activition_data = all_data[activition_need_cols]\n",
    "# activition_data.sort_values(['user_id', 'click_timestamp'], inplace=True)\n",
    "# # all_data[all_data[\"user_id\"]==249999]\n",
    "# user_act_df = pd.DataFrame(activition_data.groupby('user_id', as_index=False)[['click_article_id', 'click_timestamp']].\\\n",
    "#                             agg({'click_article_id':np.size, 'click_timestamp': {list}}).values, columns=['user_id', 'click_size', 'click_timestamp'])\n",
    "# def time_diff_mean(l):\n",
    "#     if len(l) == 1:\n",
    "#         return 1\n",
    "#     else:\n",
    "#         return np.mean([j-i for i, j in list(zip(l[:-1], l[1:]))])\n",
    "# user_act_df['time_diff_mean'] = user_act_df['click_timestamp'].apply(lambda x: time_diff_mean(x))\n",
    "# # 点击次数取倒数\n",
    "# user_act_df['click_size'] = 1 / user_act_df['click_size']\n",
    "# # 将点击事件间隔均值和点击次数分别归一化\n",
    "# user_act_df['click_size'] = (user_act_df['click_size'] - user_act_df['click_size'].min()) / (user_act_df['click_size'].max() - user_act_df['click_size'].min())\n",
    "# user_act_df['time_diff_mean'] = (user_act_df['time_diff_mean'] - user_act_df['time_diff_mean'].min()) / (user_act_df['time_diff_mean'].max() - user_act_df['time_diff_mean'].min())     \n",
    "# # 新增 用户活跃参数：active_level_parameter，再去取倒数，表示越高越活跃\n",
    "# user_act_df['active_level_parameter'] = 1/(user_act_df['click_size'] + user_act_df['time_diff_mean'])\n",
    "# user_act_df['user_id'] = user_act_df['user_id'].astype('int')\n",
    "# del user_act_df['click_timestamp']\n",
    "# user_act_df\n",
    "#################################￥￥￥￥￥￥￥￥￥￥￥￥￥￥￥￥￥￥￥￥￥￥￥￥￥￥￥￥￥￥￥\n",
    "# 活跃度计算参数的函数！\n",
    "def user_activition_level(all_data, activition_need_cols):\n",
    "    \"\"\"\n",
    "    用户活跃度参数的计算流程函数：\n",
    "    :all_data: 全数据集\n",
    "    :activition_need_cols: 计算活跃度用到的特征列，这里是点击的次数：文章的ID（做count）和点击的时间（为了做差）！\n",
    "    \"\"\"\n",
    "    user_activition_data = all_data[activition_need_cols]\n",
    "    # 接下来要对每一个用户，每一次点击时间排序；\n",
    "    user_activition_data.sort_values(['user_id', 'click_timestamp'], inplace=True)\n",
    "    # 对每一个用户groupby，统计其点击次数，以及每次点击的时间，groupby到一个格子里,agg指定统计后的形式，一个数字，一个列表存储；\n",
    "    user_act_df = pd.DataFrame(user_activition_data.groupby('user_id', as_index=False)[['click_article_id', 'click_timestamp']].\\\n",
    "                            agg({'click_article_id':np.size, 'click_timestamp': {list}}).values, columns=['user_id', 'click_size', 'click_timestamp'])\n",
    "    # 计算时间间隔的均值\n",
    "    def time_diff_mean(l):\n",
    "        if len(l) == 1:\n",
    "            return 1\n",
    "        # 分别计算相邻两次的时间差\n",
    "        # :-1表示除了最后一个往前的所有元素；1:表示除了第一个往后的所有元素；所有的时间差计算以后取平均值\n",
    "        else:\n",
    "            return np.mean([j-i for i, j in list(zip(l[:-1], l[1:]))])\n",
    "    user_act_df['time_diff_mean'] = user_act_df['click_timestamp'].apply(lambda x: time_diff_mean(x))\n",
    "    # 点击次数取到数\n",
    "    user_act_df['click_size'] = 1/user_act_df['click_size']\n",
    "    # 将点击事件间隔均值和点击次数分别归一化\n",
    "    user_act_df['click_size'] = (user_act_df['click_size'] - user_act_df['click_size'].min()) / (user_act_df['click_size'].max() - user_act_df['click_size'].min())\n",
    "    user_act_df['time_diff_mean'] = (user_act_df['time_diff_mean'] - user_act_df['time_diff_mean'].min()) / (user_act_df['time_diff_mean'].max() - user_act_df['time_diff_mean'].min())     \n",
    "    # 新增 用户活跃参数：active_level_parameter，倒过来，越大越活跃表示，\n",
    "    user_act_df['active_level_parameter'] = 1 /(user_act_df['click_size'] + user_act_df['time_diff_mean'] )\n",
    "    user_act_df['user_id'] = user_act_df['user_id'].astype('int')\n",
    "    del user_act_df['click_timestamp']\n",
    "    return user_act_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8657881f",
   "metadata": {},
   "source": [
    "- 将用户活跃度结果计算出来：user_act_fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fbe05045",
   "metadata": {},
   "outputs": [],
   "source": [
    "activition_need_cols = ['user_id', 'click_article_id', 'click_timestamp']\n",
    "user_act_fea = user_activition_level(all_data, activition_need_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364cfd5d",
   "metadata": {},
   "source": [
    "## P2.分析一下文章的点击时间和被点击次数， 衡量文章热度特征；\n",
    "- 分析一下文章被点击时间和被点击次数，区分文章热门程度；\n",
    "> 1. 根据文章进行分组， 计算点击的时间间隔平均值；\n",
    "> 2. 计算文章被点击的次数；\n",
    "> 3. 将文章点击的用户数量取倒数，并将用户数量和时间间隔归一化，相加得到文章的热度特征再取倒数，该值越大，说明文章越热；\n",
    "> 4. 最终得到的一列是click_article_id,user_num被点击次数的倒数，time_diff_mean平均点击时间间隔，hot_parameter活跃度参数！\n",
    "- 最终结果：articles_hot_fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "944f349b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def articles_hot_level(all_data, hot_need_cols):\n",
    "    \"\"\"\n",
    "    文章热门程度的\n",
    "    :all_data: 全数据集\n",
    "    :activition_need_cols: 计算文章热度用到的特征列，这里是点击的次数：文章的ID（做count）和点击的时间（为了做差）！\n",
    "    \"\"\"\n",
    "    articles_hot_data = all_data[hot_need_cols]\n",
    "    # 接下来要对每一篇文章，按每一次点击时间排序；\n",
    "    articles_hot_data.sort_values(['click_article_id', 'click_timestamp'], inplace=True)\n",
    "    # 对每一篇文章groupby，统计其被点击次数，以及每次点击的时间，groupby到一个格子里,agg指定统计后的形式，一个数字，一个列表存储；\n",
    "    article_hot_df = pd.DataFrame(articles_hot_data.groupby('click_article_id', as_index=False)[['user_id', 'click_timestamp']].\\\n",
    "                               agg({'user_id':np.size, 'click_timestamp': {list}}).values, columns=['click_article_id', 'user_num', 'click_timestamp'])\n",
    "    # 计算被点击时间间隔的均值\n",
    "    def time_diff_mean(l):\n",
    "        if len(l) == 1:\n",
    "            return 1\n",
    "        else:\n",
    "            return np.mean([j-i for i, j in list(zip(l[:-1], l[1:]))])\n",
    "    article_hot_df['time_diff_mean'] = article_hot_df['click_timestamp'].apply(lambda x: time_diff_mean(x))\n",
    "    # 被点击次数取倒数\n",
    "    article_hot_df['user_num'] = 1 / article_hot_df['user_num']\n",
    "    # 两者归一化\n",
    "    article_hot_df['user_num'] = (article_hot_df['user_num'] - article_hot_df['user_num'].min()) / (article_hot_df['user_num'].max() - article_hot_df['user_num'].min())\n",
    "    article_hot_df['time_diff_mean'] = (article_hot_df['time_diff_mean'] - article_hot_df['time_diff_mean'].min()) / (article_hot_df['time_diff_mean'].max() - article_hot_df['time_diff_mean'].min())     \n",
    "    # 新增 文章热度参数：hot_parameter，倒过来，越大越活跃表示\n",
    "    article_hot_df['hot_parameter'] = 1 /(article_hot_df['user_num'] + article_hot_df['time_diff_mean'])\n",
    "    article_hot_df['click_article_id'] = article_hot_df['click_article_id'].astype('int')\n",
    "    del article_hot_df['click_timestamp']\n",
    "    return article_hot_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caebb68d",
   "metadata": {},
   "source": [
    "- 将文章热门度结果计算出来：articles_hot_fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "35f6095d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_need_cols = ['user_id', 'click_article_id', 'click_timestamp']\n",
    "articles_hot_fea = articles_hot_level(all_data, hot_need_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e040bc1",
   "metadata": {},
   "source": [
    "# C5.特征工程环节之四——用户习惯偏好层面的数据挖掘；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57f5c89",
   "metadata": {},
   "source": [
    "- 从all_click_df就是全部点击日志中，可以看到点击环境，时间，设备，地区，类型等信息；这个是不是可以视为所谓的用户画像？\n",
    "- 1.用户的设备习惯， 这里取最常用的设备，最多的是哪个——取众数；\n",
    "- 2.用户的时间习惯： 用户习惯啥时候看新闻和物品；\n",
    "- 3.用户的爱好特征， 文章的类别、主题信息分类；\n",
    "- 4.用户文章的词数， 用户的爱好文章的字数习惯，偏爱短文章长文章；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a28eee",
   "metadata": {},
   "source": [
    "## P1.用户设备习惯特征挖掘\n",
    "- 将设备，环境，地区等信息分别统计出每个用户最多的那一个！\n",
    "- 返回user_device_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4084d937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def device_fea(all_data, cols):\n",
    "#     \"\"\"\n",
    "#     用户的最习惯设备特征类型；\n",
    "#     :all_data: 数据集\n",
    "#     :cols: 用到的特征列\n",
    "#     \"\"\"\n",
    "#     user_device_info = all_data[cols]\n",
    "    \n",
    "#     # 用众数来表示每个用户的设备信息\n",
    "#     user_device_info = user_device_info.groupby('user_id').agg(lambda x: x.value_counts().index[0]).reset_index()\n",
    "    \n",
    "#     return user_device_info\n",
    "    \n",
    "# # 设备特征(这里时间会比较长)\n",
    "# device_cols = ['user_id', 'click_environment', 'click_deviceGroup', 'click_os', 'click_country', 'click_region', 'click_referrer_type']\n",
    "# user_device_info = device_fea(all_data, device_cols)\n",
    "###################################################################￥￥￥￥￥￥￥￥￥￥￥￥￥￥￥%%%%%%%%%%%%%%%%%%%\n",
    "# 定义设备习惯需要的列\n",
    "device_need_cols = ['user_id', 'click_environment', 'click_deviceGroup', 'click_os', 'click_country', 'click_region', 'click_referrer_type']\n",
    "# 指定列后统计列上每一特征的出现“种类数”；从每一个特征中挑选出出现次数最多的！\n",
    "user_device_info = all_data[device_need_cols]\n",
    "user_device_info_df = user_device_info.groupby('user_id').agg(lambda x: x.value_counts().index[0]).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec354b81",
   "metadata": {},
   "source": [
    "## P2.用户时间习惯特征挖掘\n",
    "- 个人意见是时间戳转换成时间格式，其实换成周几+几点！\n",
    "- 返回user_timehobby_info：用户id-点击周几-点击几点-文章创建周几-文章创建几点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3e20e5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def user_time_hob_fea(all_data, cols):\n",
    "#     \"\"\"\n",
    "#     制作用户的时间习惯特征\n",
    "#     :param all_data: 数据集\n",
    "#     :param cols: 用到的特征列\n",
    "#     \"\"\"\n",
    "#     user_time_hob_info = all_data[cols]\n",
    "#     # 先把时间戳进行归一化\n",
    "#     mm = MinMaxScaler()\n",
    "#     user_time_hob_info['click_timestamp'] = mm.fit_transform(user_time_hob_info[['click_timestamp']])\n",
    "#     user_time_hob_info['created_at_ts'] = mm.fit_transform(user_time_hob_info[['created_at_ts']])\n",
    "#     user_time_hob_info = user_time_hob_info.groupby('user_id').agg('mean').reset_index()\n",
    "#     user_time_hob_info.rename(columns={'click_timestamp': 'user_time_hob1', 'created_at_ts': 'user_time_hob2'}, inplace=True)\n",
    "#     return user_time_hob_info\n",
    "# user_time_hob_cols = ['user_id', 'click_timestamp', 'created_at_ts']\n",
    "# user_time_hob_info = user_time_hob_fea(all_data, user_time_hob_cols)\n",
    "user_timehobby_need_cols = ['user_id', 'click_timestamp', 'created_at_ts']\n",
    "user_timehobby_info = all_data[user_timehobby_need_cols]\n",
    "# 单独生成一列，是点击文章在星期几\n",
    "user_timehobby_info[\"click_timestamp_1\"] = user_timehobby_info[\"click_timestamp\"].astype(\"int64\").apply(lambda x: pd.to_datetime(x,unit='ms').weekday()+1)\n",
    "# 再生成一列，是点击文章在几点\n",
    "user_timehobby_info[\"click_timestamp_2\"] = user_timehobby_info[\"click_timestamp\"].astype(\"int64\").apply(lambda x: int(datetime.strftime(pd.to_datetime(x,unit='ms'),\"%H:%M:%S\")[:2]))\n",
    "# 文章的创建时间也做同样的两个处理：\n",
    "user_timehobby_info[\"created_at_ts_1\"] = user_timehobby_info[\"created_at_ts\"].apply(lambda x: pd.to_datetime(x,unit='ms').weekday()+1)\n",
    "user_timehobby_info[\"created_at_ts_2\"] = user_timehobby_info[\"created_at_ts\"].apply(lambda x: int(datetime.strftime(pd.to_datetime(x,unit='ms'),\"%H:%M:%S\")[:2]))\n",
    "del user_timehobby_info[\"click_timestamp\"]\n",
    "del user_timehobby_info[\"created_at_ts\"]\n",
    "user_timehobby_info.columns = [\"user_id\", \"click_weekday\", \"click_hour\", \"article_crt_weekday\", \"article_crt_hour\"]\n",
    "# 找出用户最爱在周几点击，几点点击，最喜欢周几创建的文章，几点创建的文章\n",
    "user_timehobby_info_df = user_timehobby_info.groupby([\"user_id\"]).agg(lambda x: x.value_counts().index[0]).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92368529",
   "metadata": {},
   "source": [
    "## P3.用户主题类别偏好特征挖掘\n",
    "- 把每个用户的点击过得文章类别，变成一列列表特征；\n",
    "- 返回user_category_hobby_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4aecc4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def user_cat_hob_fea(all_data, cols):\n",
    "#     \"\"\"\n",
    "#     用户的主题爱好\n",
    "#     :param all_data: 数据集\n",
    "#     :param cols: 用到的特征列\n",
    "#     \"\"\"\n",
    "#     user_category_hob_info = all_data[cols]\n",
    "#     user_category_hob_info = user_category_hob_info.groupby('user_id').agg({list}).reset_index()\n",
    "#     user_cat_hob_info = pd.DataFrame()\n",
    "#     user_cat_hob_info['user_id'] = user_category_hob_info['user_id']\n",
    "#     user_cat_hob_info['cate_list'] = user_category_hob_info['category_id']\n",
    "#     return user_cat_hob_info\n",
    "# user_category_hob_cols = ['user_id', 'category_id']\n",
    "# user_cat_hob_info = user_cat_hob_fea(all_data, user_category_hob_cols)\n",
    "# 拿出相关列\n",
    "user_category_need_cols = ['user_id', 'category_id']\n",
    "user_category_hobby_info = all_data[user_category_need_cols]\n",
    "# 点击类别列表化\n",
    "user_category_hobby_info = user_category_hobby_info.groupby('user_id').agg({list}).reset_index()\n",
    "# 重新Dataframe化\n",
    "user_category_hobby_info_df = pd.DataFrame()\n",
    "user_category_hobby_info_df['user_id'] = user_category_hobby_info['user_id']\n",
    "user_category_hobby_info_df['cate_list'] = user_category_hobby_info['category_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271189da",
   "metadata": {},
   "source": [
    "## P4.用户的文章词数偏好特征挖掘\n",
    "- 很简单，用户点击过的文章词语数平均值；其实可以考虑加权重；\n",
    "- 返回user_wordscount_hobby_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b3e1b348",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_wordscount_hobby_info_df = all_data.groupby('user_id')['words_count'].agg('mean').reset_index()\n",
    "user_wordscount_hobby_info_df.rename(columns={'words_count': 'words_hbo'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce17b7e9",
   "metadata": {},
   "source": [
    "## P5.用户层面的所有信息汇总；\n",
    "- 前面处理好的用户活跃度信息：user_act_fea\n",
    "- 设备偏好：user_device_info_df\n",
    "- 时间偏好：user_timehobby_info\n",
    "- 类别偏好：user_category_hobby_info_df\n",
    "- 词数偏好：user_wordscount_hobby_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d4d30ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_feature_all_info = pd.merge(user_act_fea, user_device_info_df, on='user_id')\n",
    "user_feature_all_info = user_feature_all_info.merge(user_timehobby_info_df, on='user_id')\n",
    "user_feature_all_info = user_feature_all_info.merge(user_category_hobby_info_df, on='user_id')\n",
    "user_feature_all_info = user_feature_all_info.merge(user_wordscount_hobby_info_df, on='user_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50608f6",
   "metadata": {},
   "source": [
    "## P6.最终用户侧特征保存本地\n",
    "- 保存到本地指定路径 ：user_feature_info.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746b5e63",
   "metadata": {},
   "source": [
    "# C6.最终处理；\n",
    "- 将前面得到的\n",
    ">> 1. trn_user_item_feats_df = pd.read_csv(save_dir + 'trn_user_item_feats_df.csv')\n",
    ">> 2. tst_user_item_feats_df = pd.read_csv(save_dir + 'tst_user_item_feats_df.csv')\n",
    "- 拼接上\n",
    ">> 1. user_feature_all_info.to_csv(save_dir + 'user_feature_info.csv', index=False) \n",
    "得到trn_user_item_feats_df+tst_user_item_feats_df，并做字段顺序调整；并且对召回文章的类别分析，形成新字段；\n",
    "即召回文章的主题类别是否在用户的点击过的主题类别里面：即is_cat_hub字段；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321e719a",
   "metadata": {},
   "source": [
    "### S1.保存/读取用户特征读取；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d49207bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存好以后就可以读取了\n",
    "# user_feature_all_info.to_csv(save_dir + 'user_feature_info.csv', index=False)  \n",
    "# 把用户信息直接读入进来\n",
    "user_feature_all_info = pd.read_csv(save_dir + 'user_feature_info.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e570ea3a",
   "metadata": {},
   "source": [
    "### S2.之前特征划分的训练集+测试集拼上新制作的用户侧特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5b9d2b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取\n",
    "if os.path.exists(save_dir + 'trn_user_item_feats_df.csv'):\n",
    "    trn_user_item_feats_df = pd.read_csv(save_dir + 'trn_user_item_feats_df.csv')\n",
    "    \n",
    "if os.path.exists(save_dir + 'tst_user_item_feats_df.csv'):\n",
    "    tst_user_item_feats_df = pd.read_csv(save_dir + 'tst_user_item_feats_df.csv')\n",
    "\n",
    "if os.path.exists(save_dir + 'val_user_item_feats_df.csv'):\n",
    "    val_user_item_feats_df = pd.read_csv(save_dir + 'val_user_item_feats_df.csv')\n",
    "else:\n",
    "    val_user_item_feats_df = None\n",
    "# 拼上用户特征\n",
    "trn_user_item_feats_df = trn_user_item_feats_df.merge(user_feature_all_info, on='user_id', how='left')\n",
    "if val_user_item_feats_df is not None:\n",
    "    val_user_item_feats_df = val_user_item_feats_df.merge(user_feature_all_info, on='user_id', how='left')\n",
    "else:\n",
    "    val_user_item_feats_df = None\n",
    "tst_user_item_feats_df = tst_user_item_feats_df.merge(user_feature_all_info, on='user_id',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d38ae2",
   "metadata": {},
   "source": [
    "### S3.拼接文章物品的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "af5c033c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Mem. usage decreased to  0.25 Mb (0.0% reduction),time spend:0.00 min\n"
     ]
    }
   ],
   "source": [
    "# 重新读取也可以\n",
    "# article_info_df = get_article_info_df()\n",
    "article_info_df = reduce_mem(article_info_df)\n",
    "# 拼上文章特征\n",
    "trn_user_item_feats_df_final = trn_user_item_feats_df.merge(article_info_df, left_on='click_article_id', right_on='article_id')\n",
    "if val_user_item_feats_df is not None:\n",
    "    val_user_item_feats_df = val_user_item_feats_df.merge(article_info_df, left_on='click_article_id', right_on='article_id')\n",
    "else:\n",
    "    val_user_item_feats_df = None\n",
    "tst_user_item_feats_df_final = tst_user_item_feats_df.merge(article_info_df, left_on='click_article_id', right_on='article_id') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c8bb03a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_user_item_feats_df_final[\"cate_list_new\"] = trn_user_item_feats_df_final[\"cate_list\"].apply(lambda x: set(int(i) for i in x.replace(\"[\",\"\").replace(\"]\",\"\").replace(\" \",\"\").split(\",\")))\n",
    "tst_user_item_feats_df_final[\"cate_list_new\"] = tst_user_item_feats_df_final[\"cate_list\"].apply(lambda x: set(int(i) for i in x.replace(\"[\",\"\").replace(\"]\",\"\").replace(\" \",\"\").split(\",\")))\n",
    "trn_user_item_feats_df_final['is_cat_hab'] = trn_user_item_feats_df_final.apply(lambda x: 1 if x.category_id in x.cate_list_new else 0, axis=1)\n",
    "if val_user_item_feats_df is not None:\n",
    "    trn_user_item_feats_df_final['is_cat_hab'] = trn_user_item_feats_df_final.apply(lambda x: 1 if x.category_id in set(x.cate_list) else 0, axis=1)\n",
    "else:\n",
    "    val_user_item_feats_df = None\n",
    "tst_user_item_feats_df_final['is_cat_hab'] = tst_user_item_feats_df_final.apply(lambda x: 1 if x.category_id in x.cate_list_new else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8b862ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'click_article_id', 'sim0', 'time_diff0', 'word_diff0',\n",
       "       'sim_max', 'sim_min', 'sim_sum', 'sim_mean', 'sim_median', 'score',\n",
       "       'rank', 'label', 'click_size', 'time_diff_mean',\n",
       "       'active_level_parameter', 'click_environment', 'click_deviceGroup',\n",
       "       'click_os', 'click_country', 'click_region', 'click_referrer_type',\n",
       "       'click_weekday', 'click_hour', 'article_crt_weekday',\n",
       "       'article_crt_hour', 'cate_list', 'words_hbo', 'article_id',\n",
       "       'category_id', 'created_at_ts', 'words_count', 'cate_list_new',\n",
       "       'is_cat_hab'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_user_item_feats_df_final.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1009d379",
   "metadata": {},
   "source": [
    "## S4.调整一下顺序！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dc1f7487",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_user_item_feats_df_final_sample = trn_user_item_feats_df_final[['user_id', 'click_article_id', 'sim0', 'time_diff0', 'word_diff0',\n",
    "       'sim_max', 'sim_min', 'sim_sum', 'sim_mean', 'sim_median', 'score','rank', 'click_size', 'time_diff_mean','active_level_parameter', \n",
    "       'click_environment', 'click_deviceGroup', 'click_os', 'click_country', 'click_region', 'click_referrer_type', 'click_weekday', \n",
    "       'click_hour', 'article_crt_weekday', 'article_crt_hour',  'words_hbo', 'category_id', 'words_count', 'is_cat_hab', 'label']]\n",
    "tst_user_item_feats_df_final_sample = tst_user_item_feats_df_final[['user_id', 'click_article_id', 'sim0', 'time_diff0', 'word_diff0',\n",
    "       'sim_max', 'sim_min', 'sim_sum', 'sim_mean', 'sim_median', 'score','rank', 'click_size', 'time_diff_mean','active_level_parameter', \n",
    "       'click_environment', 'click_deviceGroup', 'click_os', 'click_country', 'click_region', 'click_referrer_type', 'click_weekday', \n",
    "       'click_hour', 'article_crt_weekday', 'article_crt_hour',  'words_hbo', 'category_id', 'words_count', 'is_cat_hab', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "e8be5a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 线下验证\n",
    "# del trn_user_item_feats_df_final_sample['cate_list']\n",
    "# del tst_user_item_feats_df_final_sample['cate_list']\n",
    "# del trn_user_item_feats_df_final_sample['cate_list_new']\n",
    "# del tst_user_item_feats_df_final_sample['cate_list_new']\n",
    "# del trn_user_item_feats_df_final_sample['article_id']\n",
    "# del tst_user_item_feats_df_final_sample['article_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bd1f5b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 结果再保存！\n",
    "trn_user_item_feats_df_final_sample.to_csv(save_dir + \"trn_user_item_feats_df_final_sample.csv\",index=False, encoding=\"utf8\")\n",
    "tst_user_item_feats_df_final_sample.to_csv(save_dir + \"tst_user_item_feats_df_final_sample.csv\",index=False, encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac579fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731b7708",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ab24db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e611bad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a054e3ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb6a851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3bba57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dc52e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27e349c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83270649",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0645066",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
